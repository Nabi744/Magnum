{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Linear Regression",
   "id": "407d96733b06a56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regression model\n",
    "Linear regression model's equation is as follows:\n",
    "\n",
    "$$f(x)=Wx+b$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted value\n",
    "- $W$ : weight\n",
    "- $b$ : bias  "
   ],
   "id": "f817a4eb8971a7d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loss function  \n",
    "Also known as objective function or cost function.  \n",
    "Loss function measures how well the model is performing.  \n",
    "\n",
    "We define loss function by using MSE(Mean Squared Error)  \n",
    "\n",
    "$$Loss(W,b)=\\frac{1}{n}\\Sigma_{i=1}^{n} (f(x_i)-y_i)^2$$"
   ],
   "id": "dc91b35eddfdfe69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimization\n",
    "\n",
    "$$\\tilde{\\theta} = {\\underset{\\theta}{argmin}} Loss(x, y; \\theta)$$  \n",
    "\n",
    "Where:\n",
    "- $Loss$ : loss function\n",
    "- $argmin$ : argument that minimizes the function\n",
    "- $\\theta$ : model parameters ($W$, $b$)\n",
    "- $\\tilde{\\theta}$ : optimal model parameters ($\\hat W$, $\\hat b$)\n",
    "- $x$ : input data\n",
    "- $y$ : target data"
   ],
   "id": "b0b1d455a9697080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Find optimal model parameters\n",
    "\n",
    "Since Loss function is a convex function, we can find the optimal model parameters by finding the point where the derivative of the loss function is zero.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta}=0$$\n",
    "\n",
    "In our case, we have two parameters $W$ and $b$.\n",
    "$$\\frac{\\partial L}{\\partial W}=0$$  \n",
    "$$\\frac{\\partial L}{\\partial b}=0$$  "
   ],
   "id": "58a6d53fa969150"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\alpha \\nabla Loss(x, y; \\theta)$$\n",
    "where:\n",
    "- $\\theta_{t}$ : model parameters at time $t$\n",
    "- $\\alpha$ : learning rate\n",
    "\n",
    "In our case, we have two parameters $W$ and $b.\n",
    "$$W_{t+1} = W_{t} - \\alpha \\nabla Loss(x, y; W)$$  \n",
    "$$b_{t+1} = b_{t} - \\alpha \\nabla Loss(x, y; b)$$  "
   ],
   "id": "a58a59fd7ec07af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "875c61ad0a90f2cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": " "
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
